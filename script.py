from pyspark import SparkConf, SparkContext

# Configuration Spark
conf = SparkConf().setAppName("WordCountWithShuffle").setMaster("local[*]")
sc = SparkContext(conf=conf)

# Lecture du fichier texte
text_file = sc.textFile("data/text.txt")

# Word Count de base
word_counts = (
    text_file
    .flatMap(lambda line: line.split())
    .map(lambda word: (word.lower(), 1))
    .reduceByKey(lambda a, b: a + b)   # ‚öôÔ∏è Shuffle #1 (reduceByKey)
)

# üî• √âtape suppl√©mentaire pour forcer d'autres shuffles
# 1Ô∏è‚É£ Trier les mots par fr√©quence d√©croissante
sorted_counts = (
    word_counts
    .map(lambda x: (x[1], x[0]))       # inverser (mot, count) ‚Üí (count, mot)
    .sortByKey(ascending=False)        # ‚öôÔ∏è Shuffle #2 (sortByKey)
)

# 2Ô∏è‚É£ Joindre avec un autre RDD (autre shuffle)
# Cr√©ons un petit RDD de cat√©gories fictives
categories = sc.parallelize([
    ("spark", "framework"),
    ("world", "general"),
    ("hello", "greeting"),
])

joined = (
    sorted_counts
    .map(lambda x: (x[1], x[0]))       # (mot, count)
    .join(categories)                  # ‚öôÔ∏è Shuffle #3 (join)
)

# Sauvegarde des r√©sultats
output_path = "output/wordcount_shuffle"
joined.saveAsTextFile(output_path)

print(f"\n‚úÖ R√©sultat sauvegard√© dans : {output_path}")
print("üìä Ouvre ton navigateur sur : http://localhost:4040 pour voir les Jobs, Stages et Shuffles\n")

# Maintenir l'UI Spark ouverte
input("‚û°Ô∏è  Appuie sur [Entr√©e] pour fermer Spark et quitter...")

sc.stop()
